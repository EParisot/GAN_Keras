{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rock_\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "import keras.callbacks\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    if dataset == \"mnist\":\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    else:\n",
    "        x_train = np.load(\"Data/\" + dataset + \".npy\")\n",
    "    x_train = (x_train.astype(np.float32) - 127.5)/127.5\n",
    "    x_train = x_train.reshape(len(x_train), 784)\n",
    "    return (x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1D_generator(rand_shape, output_shape):\n",
    "    \n",
    "    data_in = Input(shape=rand_shape, name=\"data_in\")\n",
    "    \n",
    "    x = Dense(256, kernel_initializer=initializers.RandomNormal(stddev=0.02))(data_in)       \n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Dense(512)(x)       \n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Dense(1024)(x)       \n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    out = Dense(output_shape[0], activation='tanh')(x)\n",
    "    \n",
    "    generator = Model(inputs=[data_in], outputs=[out])\n",
    "    generator.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "    generator.summary()\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1D_discriminator(input_shape):\n",
    "    \n",
    "    data_in = Input(shape=input_shape, name=\"data_in\")\n",
    "    \n",
    "    x = Dense(1024, kernel_initializer=initializers.RandomNormal(stddev=0.02))(data_in)       \n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(512)(x)       \n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(256)(x)       \n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    discriminator = Model(inputs=[data_in], outputs=[out])\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "    discriminator.summary()\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(generator, discriminator, rand_shape):\n",
    "    \n",
    "    # set generation mode\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    gan_input = Input(shape=rand_shape)\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    \n",
    "    gan = Model(inputs=[gan_input], outputs=[gan_output])\n",
    "    gan.compile(loss=\"binary_crossentropy\", optimizer=\"adadelta\")\n",
    "    \n",
    "    return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 1D GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(dataset, rand_param_size=100, epochs=10, batch_size=32, show_img=True):\n",
    "    np.random.seed(10)\n",
    "    # Load data\n",
    "    x_train = load_data(dataset)\n",
    "    # Set shapes and stuff\n",
    "    rand_shape = (rand_param_size, )\n",
    "    output_shape = x_train.shape[1:]\n",
    "    batch_count = int(x_train.shape[0] / batch_size)\n",
    "\n",
    "    # Build models\n",
    "    generator = build_1D_generator(rand_shape, output_shape)\n",
    "    discriminator = build_1D_discriminator(output_shape)\n",
    "    gan = build_gan(generator, discriminator, rand_shape)\n",
    "\n",
    "    # Train for each epoch\n",
    "    for i in range(1, epochs + 1):\n",
    "        print('-'*15, 'Epoch %d' % i, '-'*15)\n",
    "        # for each batch\n",
    "        for j in tqdm(range(batch_count)):\n",
    "            # build fake image\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, rand_param_size])\n",
    "            generated_images = generator.predict(noise)\n",
    "            # grab real images\n",
    "            image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n",
    "            # Build mixed dataset\n",
    "            X = np.concatenate([image_batch, generated_images])\n",
    "            # Build labels\n",
    "            y_dis = np.zeros(2*batch_size)\n",
    "            y_dis[:batch_size] = 0.9\n",
    "            # Discriminator train step\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(X, y_dis)\n",
    "            # Build new seeds and \"real\" labels\n",
    "            seeds = np.random.normal(0, 1, size=[batch_size, rand_param_size])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            # Generator train step\n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(seeds, y_gen)\n",
    "    \n",
    "    if show_img:\n",
    "        # build fake images\n",
    "        visu_seeds = np.random.normal(0, 1, size=[batch_size, rand_param_size])\n",
    "        built_images = generator.predict(visu_seeds)\n",
    "        built_images = built_images.reshape(built_images.shape[0], int(np.sqrt(built_images.shape[1])), int(np.sqrt(built_images.shape[1])))\n",
    "        # plot generated images\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        columns = 4\n",
    "        rows = batch_size / columns\n",
    "        for i, img in enumerate(built_images):\n",
    "            if i + 1 > 0 and i < columns * rows:\n",
    "                fig.add_subplot(rows, columns, i+1)\n",
    "                plt.imshow(img, cmap=\"Greys\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "data_in (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 784)               803600    \n",
      "=================================================================\n",
      "Total params: 1,486,352\n",
      "Trainable params: 1,486,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "data_in (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,460,225\n",
      "Trainable params: 1,460,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 3583/5571 [04:12<02:15, 14.69it/s]"
     ]
    }
   ],
   "source": [
    "train_gan(\"horse\", rand_param_size=100, epochs=20, batch_size=32, show_img=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2D_generator(rand_shape, output_shape):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2D_discriminator(input_shape):\n",
    "    \n",
    "    data_in = Input(shape=input_shape, name=\"data_in\")\n",
    "\n",
    "    x = Convolution2D(filters=64, kernel_size=32, strides=2,\n",
    "                padding='same', activation=\"relu\", use_bias=False, \n",
    "                kernel_initializer=initializers.RandomNormal(stddev=0.02))(data_in)       \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Convolution2D(filters=32, kernel_size=16, strides=2, padding='same', \n",
    "                      activation=\"relu\", use_bias=False)(x)       \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Convolution2D(filters=16, kernel_size=8, strides=2, padding='same', \n",
    "                      activation=\"relu\", use_bias=False)(x)       \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Flatten(name='flattened')(x)\n",
    "\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    discriminator = Model(inputs=[data_in], outputs=[out])\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "    return discriminator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
