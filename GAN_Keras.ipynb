{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.callbacks\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    if dataset == \"mnist\":\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    else:\n",
    "        x_train = np.load(\"Data/\" + dataset + \".npy\")\n",
    "    x_train = (x_train.astype(np.float32) - 127.5)/127.5\n",
    "    x_train = x_train.reshape(len(x_train), 784)\n",
    "    return (x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1D_generator(rand_shape, output_shape):\n",
    "    \n",
    "    data_in = Input(shape=rand_shape, name=\"data_in\")\n",
    "    \n",
    "    x = Dense(256, kernel_initializer=initializers.RandomNormal(stddev=0.02))(data_in)       \n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Dense(512)(x)       \n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Dense(1024)(x)       \n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    out = Dense(output_shape[0], activation='tanh')(x)\n",
    "    \n",
    "    generator = Model(inputs=[data_in], outputs=[out])\n",
    "    generator.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "    generator.summary()\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1D_discriminator(input_shape):\n",
    "    \n",
    "    data_in = Input(shape=input_shape, name=\"data_in\")\n",
    "    \n",
    "    x = Dense(1024, kernel_initializer=initializers.RandomNormal(stddev=0.02))(data_in)       \n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(512)(x)       \n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(256)(x)       \n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    discriminator = Model(inputs=[data_in], outputs=[out])\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "    discriminator.summary()\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(generator, discriminator, rand_shape):\n",
    "    \n",
    "    # set generation mode\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    gan_input = Input(shape=rand_shape)\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    \n",
    "    gan = Model(inputs=[gan_input], outputs=[gan_output])\n",
    "    gan.compile(loss=\"binary_crossentropy\", optimizer=\"adadelta\")\n",
    "    \n",
    "    return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 1D GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(dataset, rand_param_size=100, epochs=10, batch_size=32, show_img=True):\n",
    "    np.random.seed(10)\n",
    "    # Load data\n",
    "    x_train = load_data(dataset)\n",
    "    # Set shapes and stuff\n",
    "    rand_shape = (rand_param_size, )\n",
    "    output_shape = x_train.shape[1:]\n",
    "    batch_count = int(x_train.shape[0] / batch_size)\n",
    "\n",
    "    # Build models\n",
    "    generator = build_1D_generator(rand_shape, output_shape)\n",
    "    discriminator = build_1D_discriminator(output_shape)\n",
    "    gan = build_gan(generator, discriminator, rand_shape)\n",
    "\n",
    "    # Train for each epoch\n",
    "    for i in range(1, epochs + 1):\n",
    "        print('-'*15, 'Epoch %d' % i, '-'*15)\n",
    "        # for each batch\n",
    "        for j in tqdm(range(batch_count)):\n",
    "            # build fake image\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, rand_param_size])\n",
    "            generated_images = generator.predict(noise)\n",
    "            # grab real images\n",
    "            image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n",
    "            # Build mixed dataset\n",
    "            X = np.concatenate([image_batch, generated_images])\n",
    "            # Build labels\n",
    "            y_dis = np.zeros(2*batch_size)\n",
    "            y_dis[:batch_size] = 0.9\n",
    "            # Discriminator train step\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(X, y_dis)\n",
    "            # Build new seeds and \"real\" labels\n",
    "            seeds = np.random.normal(0, 1, size=[batch_size, rand_param_size])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            # Generator train step\n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(seeds, y_gen)\n",
    "    \n",
    "    if show_img:\n",
    "        # build fake images\n",
    "        visu_seeds = np.random.normal(0, 1, size=[batch_size, rand_param_size])\n",
    "        built_images = generator.predict(visu_seeds)\n",
    "        built_images = built_images.reshape(built_images.shape[0], int(np.sqrt(built_images.shape[1])), int(np.sqrt(built_images.shape[1])))\n",
    "        # plot generated images\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        columns = 4\n",
    "        rows = batch_size / columns\n",
    "        for i, img in enumerate(built_images):\n",
    "            if i + 1 > 0 and i < columns * rows:\n",
    "                fig.add_subplot(rows, columns, i+1)\n",
    "                plt.imshow(img, cmap=\"Greys\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "data_in (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 784)               803600    \n",
      "=================================================================\n",
      "Total params: 1,486,352\n",
      "Trainable params: 1,486,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "data_in (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,460,225\n",
      "Trainable params: 1,460,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 83/1875 [00:08<02:56, 10.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-c13468a23daf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_gan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mnist\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand_param_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_img\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-8e0d534b3735>\u001b[0m in \u001b[0;36mtrain_gan\u001b[1;34m(dataset, rand_param_size, epochs, batch_size, show_img)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;31m# Generator train step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshow_img\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rock_\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[0;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[0;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[0;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[1;32mc:\\users\\rock_\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    252\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m       extract_tensors_from_dataset=True)\n\u001b[1;32m--> 254\u001b[1;33m   \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m   \u001b[1;31m# If `model._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m   \u001b[1;31m# at this point because of the check above.  `train_on_batch` is being run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rock_\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[1;34m(input, name, out_type)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m   \"\"\"\n\u001b[1;32m--> 458\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rock_\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[1;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rock_\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[1;34m(input, out_type, name)\u001b[0m\n\u001b[0;32m   8958\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m   8959\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Shape\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8960\u001b[1;33m         name, _ctx._post_execution_callbacks, input, \"out_type\", out_type)\n\u001b[0m\u001b[0;32m   8961\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8962\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_gan(\"mnist\", rand_param_size=100, epochs=2, batch_size=32, show_img=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_2D_data(dataset):\n",
    "    if dataset == \"mnist\":\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    else:\n",
    "        x_train = np.load(\"Data/\" + dataset + \".npy\")\n",
    "    x_train = (x_train.astype(np.float32) - 127.5)/127.5\n",
    "    x_train = np.expand_dims(x_train, axis=3)\n",
    "    return (x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2D_generator(rand_shape, output_shape):\n",
    "    \n",
    "    depth = 64\n",
    "    \n",
    "    data_in = Input(shape=rand_shape, name=\"data_in\")\n",
    "    \n",
    "    x = Dense(output_shape[0] * output_shape[1] * output_shape[2], \n",
    "              kernel_initializer=initializers.RandomNormal(stddev=0.02))(data_in)       \n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = Reshape(output_shape)(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(depth // 2, 5, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = UpSampling2D()(x)\n",
    "    x = Conv2DTranspose(depth // 4, 5, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = Conv2D(depth // 8, 5, strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "     \n",
    "    out = Conv2D(1, 5, strides=2, padding='same', activation=\"sigmoid\")(x)\n",
    "    \n",
    "    generator = Model(inputs=[data_in], outputs=[out])\n",
    "    generator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    generator.summary()\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2D_discriminator(input_shape):\n",
    "    \n",
    "    data_in = Input(shape=input_shape, name=\"data_in\")\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=32, strides=2,\n",
    "                padding='same', activation=\"relu\", use_bias=False, \n",
    "                kernel_initializer=initializers.RandomNormal(stddev=0.02))(data_in)       \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(filters=32, kernel_size=16, strides=2, padding='same', \n",
    "                      activation=\"relu\", use_bias=False)(x)       \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(filters=16, kernel_size=8, strides=2, padding='same', \n",
    "                      activation=\"relu\", use_bias=False)(x)       \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Flatten(name='flattened')(x)\n",
    "\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    discriminator = Model(inputs=[data_in], outputs=[out])\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    discriminator.summary()\n",
    "\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2D_gan(generator, discriminator, rand_shape):\n",
    "    \n",
    "    # set generation mode\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    gan_input = Input(shape=rand_shape)\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    \n",
    "    gan = Model(inputs=[gan_input], outputs=[gan_output])\n",
    "    gan.compile(loss=\"binary_crossentropy\", optimizer=\"adadelta\")\n",
    "    \n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2D_gan(dataset, rand_param_size=100, epochs=10, batch_size=32, show_img=True):\n",
    "    np.random.seed(10)\n",
    "    # Load data\n",
    "    x_train = load_2D_data(dataset)\n",
    "    # Set shapes and stuff\n",
    "    rand_shape = (rand_param_size, )\n",
    "    output_shape = x_train.shape[1:]\n",
    "    batch_count = int(x_train.shape[0] / batch_size)\n",
    "\n",
    "    # Build models\n",
    "    generator = build_2D_generator(rand_shape, output_shape)\n",
    "    discriminator = build_2D_discriminator(output_shape)\n",
    "    gan = build_2D_gan(generator, discriminator, rand_shape)\n",
    "\n",
    "    # Train for each epoch\n",
    "    for i in range(1, epochs + 1):\n",
    "        print('-'*15, 'Epoch %d' % i, '-'*15)\n",
    "        # for each batch\n",
    "        for j in tqdm(range(batch_count)):\n",
    "            # build fake image\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, rand_param_size])\n",
    "            generated_images = generator.predict(noise)\n",
    "            # grab real images\n",
    "            image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n",
    "            # Build mixed dataset\n",
    "            X = np.concatenate([image_batch, generated_images])\n",
    "            # Build labels\n",
    "            y_dis = np.zeros(2*batch_size)\n",
    "            y_dis[:batch_size] = 0.9\n",
    "            # Discriminator train step\n",
    "            discriminator.trainable = True\n",
    "            discriminator.train_on_batch(X, y_dis)\n",
    "            # Build new seeds and \"real\" labels\n",
    "            seeds = np.random.normal(0, 1, size=[batch_size, rand_param_size])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            # Generator train step\n",
    "            discriminator.trainable = False\n",
    "            gan.train_on_batch(seeds, y_gen)\n",
    "    \n",
    "    if show_img:\n",
    "        # build fake images\n",
    "        visu_seeds = np.random.normal(0, 1, size=[batch_size, rand_param_size])\n",
    "        built_images = generator.predict(visu_seeds)\n",
    "        built_images = built_images.reshape(built_images.shape[0], int(np.sqrt(built_images.shape[1])), int(np.sqrt(built_images.shape[1])))\n",
    "        # plot generated images\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        columns = 4\n",
    "        rows = batch_size / columns\n",
    "        for i, img in enumerate(built_images):\n",
    "            if i + 1 > 0 and i < columns * rows:\n",
    "                fig.add_subplot(rows, columns, i+1)\n",
    "                plt.imshow(img, cmap=\"Greys\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "data_in (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               79184     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2 (None, 56, 56, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 56, 56, 32)        832       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2 (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 112, 112, 16)      12816     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 56, 56, 8)         3208      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 56, 56, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 28, 28, 1)         201       \n",
      "=================================================================\n",
      "Total params: 96,241\n",
      "Trainable params: 96,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "data_in (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 14, 14, 64)        65536     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 7, 7, 32)          524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 7, 7, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 4, 4, 16)          32768     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 4, 4, 16)          64        \n",
      "_________________________________________________________________\n",
      "flattened (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 623,297\n",
      "Trainable params: 623,073\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/937 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 1/937 [00:03<59:05,  3.79s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 2/937 [00:07<58:40,  3.77s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 3/937 [00:11<58:45,  3.77s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 4/937 [00:15<58:27,  3.76s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|          | 5/937 [00:18<57:47,  3.72s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|          | 6/937 [00:22<57:25,  3.70s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|          | 7/937 [00:25<57:17,  3.70s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|          | 8/937 [00:29<57:26,  3.71s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-14901de9acd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_2D_gan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mnist\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand_param_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_img\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-61dac6b1f446>\u001b[0m in \u001b[0;36mtrain_2D_gan\u001b[1;34m(dataset, rand_param_size, epochs, batch_size, show_img)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;31m# Generator train step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshow_img\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rock_\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[0;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[0;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[0;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[1;32mc:\\users\\rock_\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rock_\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[0;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[0;32m    312\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rock_\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m           \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[0;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[1;32mc:\\users\\rock_\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rock_\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32mc:\\users\\rock_\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rock_\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DBackpropInputGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m     63\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m           data_format=op.get_attr(\"data_format\").decode())\n\u001b[0m\u001b[0;32m     66\u001b[0m   ]\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rock_\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1021\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data_format\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1023\u001b[1;33m         \"dilations\", dilations)\n\u001b[0m\u001b[0;32m   1024\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_2D_gan(\"mnist\", rand_param_size=100, epochs=2, batch_size=64, show_img=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
